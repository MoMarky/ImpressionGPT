{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **ImpressionGPT**"
      ],
      "metadata": {
        "id": "k4q-J2STCdFp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "package install"
      ],
      "metadata": {
        "id": "WbrLN_j1CwlT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLQ9hg90giZt"
      },
      "outputs": [],
      "source": [
        "!pip install -q openai\n",
        "!pip install -q evaluate rouge_score\n",
        "!pip install -q rouge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMaKHaVwglh5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import openai\n",
        "import concurrent.futures\n",
        "from tqdm import tqdm\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import time\n",
        "from rouge import Rouge\n",
        "import rouge\n",
        "\n",
        "# This is your key\n",
        "openai.api_key = \"This is your key\"\n",
        "print(openai.api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGK3_tZjrecY"
      },
      "source": [
        "# GPT_api_dynamic_prompt_with_interactive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_xZ4zB1vKkY"
      },
      "outputs": [],
      "source": [
        "def chestGPT_summary_near_sample_with_interactive_v6(text, near_samples=None, interactive=False, former_good_response=None, former_bad_response=None):\n",
        "  # One good response and One Bad response.\n",
        "  max_tries = 2\n",
        "\n",
        "  dynamic_messages = [\n",
        "      {\"role\": \"system\", \"content\": \"You are a chest radiologist that identify the main findings and diagnosis or impression based on the given\\\n",
        "        FINDINGS section of the chest X-ray report, which details the radiologists' assessment of the chest X-ray image. \\\n",
        "        Please ensure that your response is concise and does not exceed the length of the FINDINGS.\"}]\n",
        "  if near_samples is not None:\n",
        "    dynamic_messages.append({\"role\": \"user\", \"content\": \"Here are some examples. Please learn how to write IMPRESSION in these examples, and \\\n",
        "    pay particular attention to the consistent use of phrases in these below examples.\"})\n",
        "    for near in near_samples:\n",
        "      dynamic_messages.append({\"role\": \"user\", \"content\": \"What are the main findings and diagnosis or impression based on the given Finding in chest X-ray report:\\\n",
        "          \\nFINDINGS:\\n{}\".format(near[\"finding\"])})\n",
        "      dynamic_messages.append({\"role\": \"assistant\", \"content\": \"IMPRESSION:\\n{}\".format(near[\"impression\"])})\n",
        "\n",
        "  for try_number in range(max_tries):\n",
        "    try:\n",
        "      if not interactive:\n",
        "        dynamic_messages.append({\"role\": \"user\", \"content\": \"What are the main findings and diagnosis or impression based on the given Finding in \\\n",
        "          chest X-ray report: \\nFINDINGS: {}\".format(text)})\n",
        "        # print(dynamic_messages)\n",
        "        response = openai.ChatCompletion.create(\n",
        "          # model=\"gpt-4\",\n",
        "          model=\"gpt-3.5-turbo\",\n",
        "          messages=dynamic_messages )\n",
        "      else:\n",
        "        dynamic_messages.append({\"role\": \"user\", \"content\": \"What are the main findings and diagnosis or impression based on the given Finding in chest \\\n",
        "                        X-ray report: \\nFINDINGS:\\n{}\".format(text)})\n",
        "        if former_bad_response is not None:\n",
        "          for index, item in enumerate(former_bad_response):\n",
        "            dynamic_messages.append({\"role\": \"user\", \"content\": \"Below is a bad impression of the FINDINGS above:\"})\n",
        "            dynamic_messages.append({\"role\": \"assistant\", \"content\": \"IMPRESSION:\\n{}\".format(item[\"summarize\"])})\n",
        "\n",
        "          if len(former_good_response) == 0:\n",
        "            dynamic_messages.append({\"role\": \"user\", \"content\": \"Please give another impression of the FINDINGS above. \\\n",
        "            It is important to note that your answer should avoid being consistent with the bad impression above. \\\n",
        "            Please pay particular attention to the consistent use of phrases in the above examples\"})\n",
        "          # else:\n",
        "          #   dynamic_messages.append({\"role\": \"user\", \"content\": \"Please give another impression based on the above FINDINGS.\\\n",
        "          #         Try to refer to the good response above and avoid the same bad response as above.\"})\n",
        "        \n",
        "        if former_good_response is not None:\n",
        "          for index, item in enumerate(former_good_response):\n",
        "            dynamic_messages.append({\"role\": \"user\", \"content\": \"Below is an excellent impression of the FINDINGS above:\"})\n",
        "            dynamic_messages.append({\"role\": \"assistant\", \"content\": \"IMPRESSION:\\n{}\".format(item[\"summarize\"])})\n",
        "            # dynamic_messages.append({\"role\": \"user\", \"content\": \"This is a excellent impression, please give another clearer impression in this format.\"})\n",
        "          if len(former_bad_response) == 0:\n",
        "            dynamic_messages.append({\"role\": \"user\", \"content\": \"This is an excellent impression, \\\n",
        "              please give another impression in this format, and do not exceed the length of this impression.\\\n",
        "              Please pay particular attention to the consistent use of phrases in the above examples\"})\n",
        "          else:\n",
        "            dynamic_messages.append({\"role\": \"user\", \"content\": \"Please give another impression of the FINDINGS above. \\\n",
        "              It is important to note that your answer should avoid being consistent with the bad impression above, \\\n",
        "              but should be consistent with the excellent impression above, and do not exceed the length of the excellent impression. \\\n",
        "              And please pay particular attention to the consistent use of phrases in the above examples\"})\n",
        "\n",
        "\n",
        "        # print(dynamic_messages)\n",
        "        # time.sleep(20)\n",
        "        response = openai.ChatCompletion.create(\n",
        "          # model=\"gpt-4\",\n",
        "          model=\"gpt-3.5-turbo\",\n",
        "          messages=dynamic_messages\n",
        "        )\n",
        "        # time.sleep(20)\n",
        "      return response['choices'][0]['message']['content']\n",
        "\n",
        "    except openai.error.APIError as e:\n",
        "      if try_number == max_tries - 1:\n",
        "        print('APIError')\n",
        "        return '\\n'\n",
        "      else:\n",
        "        time.sleep(0.1)\n",
        "    except openai.error.Timeout as e:\n",
        "      if try_number == max_tries - 1:\n",
        "        print('Timeout')\n",
        "        return '\\n'\n",
        "      else:\n",
        "        time.sleep(0.1)\n",
        "    except openai.error.APIConnectionError as e:\n",
        "      if try_number == max_tries - 1:\n",
        "        print('APIConnectionError')\n",
        "        return '\\n'\n",
        "      else:\n",
        "        time.sleep(0.1)\n",
        "    except openai.error.RateLimitError as e:\n",
        "      if try_number == max_tries - 1:\n",
        "        print('RateLimitError')\n",
        "        return '\\n'\n",
        "      else:\n",
        "        time.sleep(0.1)\n",
        "    except openai.error.InvalidRequestError as e:\n",
        "      if try_number == max_tries - 1:\n",
        "        print('InvalidRequestError')\n",
        "        return -2\n",
        "      else:\n",
        "        time.sleep(0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nbf9THTwG8qH"
      },
      "source": [
        "# random_sample_selection_v2\n",
        "\n",
        "similar search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kbsl4kI0sOqU"
      },
      "outputs": [],
      "source": [
        "def random_sample_selection_v2(test_sample_label, label_space, m=2000, k=20):\n",
        "  # MIMIC-CXR\n",
        "  dist = []\n",
        "  # study_ids = []\n",
        "  randon_choice_corpus = label_space\n",
        "  # randon_choice_corpus = label_space.sample(n=m)\n",
        "  for index, row in randon_choice_corpus.iterrows():\n",
        "    # print(row[2:])\n",
        "    # if (np.array(row[2:]) == example_label).all() and row[\"study_id\"] != test_study_id:\n",
        "    label = np.array(row[2:])\n",
        "    # label[np.isnan(label)] = 2\n",
        "    eucliDist = np.sqrt(sum(np.power((label - test_sample_label), 2)))\n",
        "    dist.append(eucliDist)\n",
        "    # study_ids.append(row[\"study_id\"])\n",
        "    # if np.allclose(np.array(row[2:]), example_label) and row[\"study_id\"] != test_study_id:\n",
        "    #   same_samples.append(row[\"study_id\"])\n",
        "  \n",
        "  np_dist_scores = np.array(dist)\n",
        "  sort_index = np.argsort(np_dist_scores)\n",
        "\n",
        "  bottom_index = sort_index[:k]\n",
        "  best_examples, best_dist, best_label = [], [], []\n",
        "  for index in bottom_index:\n",
        "    # if randon_choice_corpus.iloc[index][\"study_id\"] != study_ids[index]:\n",
        "    #   print(randon_choice_corpus.iloc[index][\"study_id\"], study_ids[index], index)\n",
        "      \n",
        "    best_examples.append(randon_choice_corpus.iloc[index][\"study_id\"])\n",
        "    best_dist.append(dist[index])\n",
        "    # best_label.append(np.array(randon_choice_corpus.iloc[index][2:]))\n",
        "\n",
        "  # print(best_dist)\n",
        "  return best_examples\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eq1Ooh1w28tg"
      },
      "outputs": [],
      "source": [
        "def random_sample_selection_v2_for_openI(test_sample_label, label_space, m=2000, k=20):\n",
        "  # OpenI\n",
        "  dist = []\n",
        "  randon_choice_corpus = label_space\n",
        "\n",
        "  for index, row in randon_choice_corpus.iterrows():\n",
        "    # print(row[2:])\n",
        "    label = np.array(row[1:])\n",
        "    eucliDist = np.sqrt(sum(np.power((label - test_sample_label), 2)))\n",
        "    dist.append(eucliDist)\n",
        "\n",
        "  np_dist_scores = np.array(dist)\n",
        "  sort_index = np.argsort(np_dist_scores)\n",
        "\n",
        "  bottom_index = sort_index[:k]\n",
        "  best_examples, best_dist, best_label = [], [], []\n",
        "  for index in bottom_index:\n",
        "\n",
        "    best_examples.append(randon_choice_corpus.iloc[index][\"ids\"])\n",
        "    best_dist.append(dist[index])\n",
        "\n",
        "  return best_examples\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EqoyNwWpGd0"
      },
      "source": [
        "# Test One Csv\n",
        "\n",
        "only compute local csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrtlz7eJpLB2"
      },
      "outputs": [],
      "source": [
        "import os.path as osp\n",
        "import math as Math\n",
        "import pandas as pd\n",
        "\n",
        "df_test_csv = pd.read_csv(\"Local CSV Path\")\n",
        "rouge = Rouge()\n",
        "\n",
        "rouge_e = evaluate.load('rouge')\n",
        "\n",
        "rouge_scores = []\n",
        "for index, row in df_test_csv.iterrows():\n",
        "    label = row['impression\\n']\n",
        "    prediction = row['summary']\n",
        "    prediction = prediction.replace(\"IMPRESSION:\", \"\")\n",
        "    score = rouge_e.compute(predictions=[prediction], references=[label], rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
        "    rouge_scores.append(score)\n",
        "\n",
        "\n",
        "# mean\n",
        "mean_rouge1 = sum([score['rouge1'] for score in rouge_scores]) / len(rouge_scores)\n",
        "mean_rouge2 = sum([score['rouge2'] for score in rouge_scores]) / len(rouge_scores)\n",
        "mean_rougeL = sum([score['rougeL'] for score in rouge_scores]) / len(rouge_scores)\n",
        "\n",
        "print(f'R-1: {mean_rouge1:.4f}', f'R-2: {mean_rouge2:.4f}', f'R-L: {mean_rougeL:.4f}')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQ4uD6kZsAci"
      },
      "source": [
        "# one good and one bad\n",
        "test MIMIC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2bWnOq5sFzp"
      },
      "outputs": [],
      "source": [
        "from rouge import Rouge\n",
        "import os.path as osp\n",
        "import os\n",
        "rouge = Rouge()\n",
        "\n",
        "\n",
        "root = \"Root path of saving CSV\"\n",
        "\n",
        "# MIMIC-CXR offical training split.\n",
        "train_data = pd.read_csv(\"Path of mimic_report_sections_train_v4.csv\") \n",
        "# CheXpert is used to obtain the train data labels.\n",
        "train_label_space = pd.read_csv('Path of train_v4_labels.csv')\n",
        "\n",
        "# MIMIC-CXR offical testing split. CheXpert is used to obtain the label.\n",
        "test_label_space = pd.read_csv('Path of test_v6_labels.csv')\n",
        "\n",
        "\n",
        "\n",
        "test_label_study_ids = []\n",
        "for index, row in test_label_space.iterrows():\n",
        "  test_label_study_ids.append(row[\"study_id\"])\n",
        "\n",
        "\n",
        "# Pre-defined para\n",
        "interactive = True\n",
        "near_k_samples = 15\n",
        "interactive_times = 17\n",
        "rouge_thre = 0.7\n",
        "rouge_type = \"rouge-1\"\n",
        "\n",
        "\"\"\"\n",
        "Below is the best paras for now:\n",
        "interactive = True\n",
        "near_k_samples = 15\n",
        "interactive_times = 17\n",
        "rouge_thre = 0.7\n",
        "rouge_type = \"rouge-1\"\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def process_row_with_random_choice_samples(args):\n",
        "    row_index, row = args\n",
        "\n",
        "    test_subject_id, test_study_id = row['subject_id'], row['study_id']\n",
        "    test_findings, test_impression = row[\"finding\"], row[\"impression\"]\n",
        "\n",
        "    # Test label vector\n",
        "    test_sample_label = np.array(test_label_space.iloc[test_label_study_ids.index(test_study_id)][2:])\n",
        "\n",
        "    # Similar Search. Return Top-near_k_samples samples' id.  m is the size of our corpus. randomly sample from all training data.\n",
        "    near_study_ids = random_sample_selection_v2(test_sample_label, train_label_space, m=10000, k=near_k_samples)\n",
        "\n",
        "    # Get the similar report Findings and Impression based on 'near_study_ids'\n",
        "    near_samples = []\n",
        "    for index, train_row in train_data.iterrows():\n",
        "      if len(near_study_ids) == 0:\n",
        "        break\n",
        "      if int(train_row[\"study_id\"]) in near_study_ids:\n",
        "        near_samples.append({\"finding\": train_data.iloc[index][\"finding\"], \"impression\": train_data.iloc[index][\"impression\"]})\n",
        "        near_study_ids.remove(train_row[\"study_id\"])\n",
        "\n",
        "    good_reponse = []\n",
        "    bad_reponse = []\n",
        "    former_socre = 0\n",
        "    former_bad_socre = 0\n",
        "    all_response_score, all_response = [], []\n",
        "    try_count = 0\n",
        "    while True:\n",
        "      if len(good_reponse) == 0 and len(bad_reponse) == 0:\n",
        "        reponse = chestGPT_summary_near_sample_with_interactive_v4(test_findings, near_samples=near_samples)\n",
        "      else:\n",
        "        reponse = chestGPT_summary_near_sample_with_interactive_v4(test_findings, near_samples=near_samples, interactive=interactive, former_good_response=good_reponse, former_bad_response=bad_reponse)\n",
        "     \n",
        "      fotmatted_response = reponse.replace(\"IMPRESSION:\", \"\")\n",
        "\n",
        "      # evaluate response\n",
        "      compare_scores = []\n",
        "      for near_sa in near_samples:\n",
        "        scores = rouge.get_scores([fotmatted_response], [near_sa[\"impression\"]])\n",
        "        single_score = scores[0][rouge_type][\"f\"]\n",
        "        compare_scores.append(single_score)\n",
        "      # print(compare_scores)\n",
        "      score = np.mean(np.array(compare_scores))\n",
        "\n",
        "      all_response_score.append(score)\n",
        "      all_response.append(fotmatted_response)\n",
        "      try_count += 1\n",
        "\n",
        "      # if it is good than former\n",
        "      if score >= rouge_thre and score > former_socre:\n",
        "        former_socre = score\n",
        "        good_reponse.clear() # clear the list after use evaluted response\n",
        "        good_reponse.append({\"summarize\": fotmatted_response, \"score\": score})\n",
        "\n",
        "      # if it is bad than former\n",
        "      if score < rouge_thre - 0.1 and score < former_socre:\n",
        "        former_bad_socre = score\n",
        "        bad_reponse.clear()\n",
        "        bad_reponse.append({\"summarize\": fotmatted_response, \"score\": score})\n",
        "      if try_count > interactive_times:\n",
        "        break\n",
        "\n",
        "    # Finally choose a best score response\n",
        "    max_score_index = all_response_score.index(max(all_response_score))\n",
        "    max_score = all_response_score[max_score_index]\n",
        "    final_best_response = all_response[max_score_index]\n",
        "    # print(\"max score=\", max_score, \"\\n\", final_best_response)\n",
        "    # print(\"max score=\", max_score, \"\\n\")\n",
        "    return row_index, final_best_response\n",
        "\n",
        "# mid_path is the experiment name\n",
        "mid_path = \"testV6_IntactV4_near{}_Ttrain_i{}r{}_{}\".format(near_k_samples, interactive_times, rouge_thre, rouge_type)\n",
        "save_root = osp.join(root, mid_path)\n",
        "if not osp.exists(save_root):\n",
        "  os.makedirs(save_root) \n",
        "print(save_root)\n",
        "for loop_index in range(7):\n",
        "  print(time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "  Test_start = loop_index * 200\n",
        "  Test_end = (loop_index + 1) * 200\n",
        "  print(Test_start, Test_end)\n",
        "  df_test_csv = pd.read_csv('Path of mimic_report_sections_test_v6.csv')\n",
        "  df_test_csv = df_test_csv[Test_start:Test_end]\n",
        "\n",
        "  # parallel\n",
        "  with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    # time.sleep(20)\n",
        "    results = dict(tqdm(executor.map(process_row_with_random_choice_samples, df_test_csv.iterrows()), total=len(df_test_csv)))\n",
        "\n",
        "  # same order as the input\n",
        "  df_test_csv['summary'] = df_test_csv.index.map(results.get)\n",
        "\n",
        "  # out_file_name is name of split csv\n",
        "  out_file_name = \"test{}_{}V6_IntactV4_near{}_Ttrain_i{}r{}_{}.csv\".format(Test_start, Test_end, near_k_samples, interactive_times, rouge_thre, rouge_type)\n",
        "  output_file = osp.join(save_root, \"{}\".format(out_file_name))\n",
        "  df_test_csv.to_csv(output_file, index=False)\n",
        "\n",
        "\n",
        "  rouge_e = evaluate.load('rouge')\n",
        "\n",
        "  rouge_scores = []\n",
        "  for index, row in df_test_csv.iterrows():\n",
        "      label = row['impression']\n",
        "      prediction = row['summary']\n",
        "      prediction = prediction.replace(\"IMPRESSION:\", \"\")\n",
        "      score = rouge_e.compute(predictions=[prediction], references=[label], rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
        "      rouge_scores.append(score)\n",
        "\n",
        "  # mean\n",
        "  mean_rouge1 = sum([score['rouge1'] for score in rouge_scores]) / len(rouge_scores)\n",
        "  mean_rouge2 = sum([score['rouge2'] for score in rouge_scores]) / len(rouge_scores)\n",
        "  mean_rougeL = sum([score['rougeL'] for score in rouge_scores]) / len(rouge_scores)\n",
        "\n",
        "  print(f'R-1: {mean_rouge1:.4f}', f',R-2: {mean_rouge2:.4f}', f',R-L: {mean_rougeL:.4f}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuHRE0ir1dNw"
      },
      "source": [
        "# one good and one bad\n",
        "test OpenI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfUW17is1jrW"
      },
      "outputs": [],
      "source": [
        "from rouge import Rouge\n",
        "\n",
        "rouge = Rouge()\n",
        "\n",
        "interactive = True\n",
        "interactive_times = 20\n",
        "rouge_thre = 0.85\n",
        "near_k_samples = 18\n",
        "rouge_type = \"rouge-1\"\n",
        "test_fold = 0\n",
        "\n",
        "df_test_csv = pd.read_csv('Path of test_data.csv'.format(test_fold, test_fold))\n",
        "train_data = pd.read_csv(\"Path of train_data.csv\".format(test_fold, test_fold))\n",
        "test_label_space = pd.read_csv(\"Path of test_label.csv\".format(test_fold, test_fold))\n",
        "train_label_space = pd.read_csv(\"Path of train_label.csv\".format(test_fold, test_fold))\n",
        "\n",
        "test_len = len(df_test_csv)\n",
        "test_label_study_ids = []\n",
        "for index, row in test_label_space.iterrows():\n",
        "  test_label_study_ids.append(row[\"ids\"])\n",
        "\n",
        "# use_near_sample = False\n",
        "# use_near_sample = True\n",
        "\n",
        "#chestGPT_summary_near_sample_with_interactive(text, near_samples=None, interactive=False, former_good_response=None, former_bad_response=None):\n",
        "\n",
        "\n",
        "def process_row_with_random_choice_samples(args):\n",
        "    row_index, row = args\n",
        "\n",
        "    test_id = row['ids']\n",
        "    test_findings, test_impression = row[\"finding\"], row[\"impression\\n\"]\n",
        "\n",
        "    test_sample_label = np.array(test_label_space.iloc[test_label_study_ids.index(test_id)][1:])\n",
        "\n",
        "    near_study_ids = random_sample_selection_v2_for_openI(test_sample_label, train_label_space, m=test_len, k=near_k_samples)\n",
        "\n",
        "    near_samples = []\n",
        "    for index, train_row in train_data.iterrows():\n",
        "      if len(near_study_ids) == 0:\n",
        "        break\n",
        "      if int(train_row[\"ids\"]) in near_study_ids:\n",
        "        near_samples.append({\"finding\": train_data.iloc[index][\"finding\"], \"impression\": train_data.iloc[index][\"impression\\n\"]})\n",
        "        near_study_ids.remove(train_row[\"ids\"])\n",
        "\n",
        "    good_reponse = []\n",
        "    bad_reponse = []\n",
        "    former_socre = 0\n",
        "    former_bad_socre = 0\n",
        "    all_response_score, all_response = [], []\n",
        "    try_count = 0\n",
        "    while True:\n",
        "      if len(good_reponse) == 0 and len(bad_reponse) == 0:\n",
        "        reponse = chestGPT_summary_near_sample_with_interactive_v4(test_findings, near_samples=near_samples)\n",
        "      else:\n",
        "        reponse = chestGPT_summary_near_sample_with_interactive_v4(test_findings, near_samples=near_samples, interactive=interactive, former_good_response=good_reponse, former_bad_response=bad_reponse)\n",
        "     \n",
        "      fotmatted_response = reponse.replace(\"IMPRESSION:\", \"\")\n",
        "      \n",
        "      compare_scores = []\n",
        "      for near_sa in near_samples:\n",
        "        scores = rouge.get_scores([fotmatted_response], [near_sa[\"impression\"]])\n",
        "        single_score = scores[0][rouge_type][\"f\"]\n",
        "        compare_scores.append(single_score)\n",
        "      # print(compare_scores)\n",
        "      score = np.mean(np.array(compare_scores))\n",
        "\n",
        "      all_response_score.append(score)\n",
        "      all_response.append(fotmatted_response)\n",
        "      try_count += 1\n",
        "      if score >= rouge_thre and score > former_socre:\n",
        "        former_socre = score\n",
        "        good_reponse.clear()\n",
        "        good_reponse.append({\"summarize\": fotmatted_response, \"score\": score})\n",
        "      if score < rouge_thre - 0.1 and score < former_socre:\n",
        "        former_bad_socre = score\n",
        "        bad_reponse.clear()\n",
        "        bad_reponse.append({\"summarize\": fotmatted_response, \"score\": score})\n",
        "      if try_count > interactive_times:\n",
        "        break\n",
        "\n",
        "    max_score_index = all_response_score.index(max(all_response_score))\n",
        "    max_score = all_response_score[max_score_index]\n",
        "    final_best_response = all_response[max_score_index]\n",
        "    # print(\"max score=\", max_score, \"\\n\", final_best_response)\n",
        "    # print(\"max score=\", max_score, \"\\n\")\n",
        "    return row_index, final_best_response\n",
        "\n",
        "# parallel\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "  results = dict(tqdm(executor.map(process_row_with_random_choice_samples, df_test_csv.iterrows()), total=test_len))\n",
        "\n",
        "# same order as the input\n",
        "df_test_csv['summary'] = df_test_csv.index.map(results.get)\n",
        "# df_test_csv['summary'] = df_test_csv.index.map(dict(results))\n",
        "\n",
        "out_file_name = \"fold{}_IntactV4_near{}_Tt_1Gd1Bd_i{}r{}_{}.csv\".format(test_fold, near_k_samples, interactive_times, rouge_thre, rouge_type)\n",
        "output_file = \"Root Path/{}\".format(out_file_name)\n",
        "df_test_csv.to_csv(output_file, index=False)\n",
        "\n",
        "\n",
        "rouge_e = evaluate.load('rouge')\n",
        "\n",
        "rouge_scores = []\n",
        "for index, row in df_test_csv.iterrows():\n",
        "    label = row['impression\\n']\n",
        "    prediction = row['summary']\n",
        "    prediction = prediction.replace(\"IMPRESSION:\", \"\")\n",
        "    score = rouge_e.compute(predictions=[prediction], references=[label], rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
        "    rouge_scores.append(score)\n",
        "\n",
        "\n",
        "# mean\n",
        "mean_rouge1 = sum([score['rouge1'] for score in rouge_scores]) / len(rouge_scores)\n",
        "mean_rouge2 = sum([score['rouge2'] for score in rouge_scores]) / len(rouge_scores)\n",
        "mean_rougeL = sum([score['rougeL'] for score in rouge_scores]) / len(rouge_scores)\n",
        "\n",
        "print(out_file_name)\n",
        "print(f'R-1: {mean_rouge1:.4f}', f'R-2: {mean_rouge2:.4f}', f'R-L: {mean_rougeL:.4f}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TUjOgkrE1G43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **One Good And N Bad**\n",
        "\n",
        "# Mimic data "
      ],
      "metadata": {
        "id": "oAE2l9sK1MuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path as osp\n",
        "import pandas as pd\n",
        "\n",
        "root = \"Root path of saving CSV\"\n",
        "\n",
        "# MIMIC-CXR offical training split.\n",
        "train_data = pd.read_csv(\"Path of mimic_report_sections_train_v4.csv\") \n",
        "# CheXpert is used to obtain the train data labels.\n",
        "train_label_space = pd.read_csv('Path of train_v4_labels.csv')\n",
        "\n",
        "# MIMIC-CXR offical testing split. CheXpert is used to obtain the label.\n",
        "test_label_space = pd.read_csv('Path of test_v6_labels.csv')\n",
        "\n",
        "\n",
        "test_label_study_ids = []\n",
        "for index, row in test_label_space.iterrows():\n",
        "  test_label_study_ids.append(row[\"study_id\"])\n",
        "\n",
        "\n",
        "# Pre-defined parameters\n",
        "interactive = True\n",
        "interactive_times = 17\n",
        "rouge_thre = 0.7\n",
        "one_near_k_samples = 200\n",
        "two_near_k_samples = 14\n",
        "rouge_type = \"rouge-1\"\n",
        "\n",
        "\n",
        "print(\"Start at\", time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "\n",
        "def process_row_with_random_choice_samples(args):\n",
        "    row_index, row = args\n",
        "\n",
        "    test_subject_id, test_study_id = row['subject_id'], row['study_id']\n",
        "    test_findings, test_impression = row[\"findings\"], row[\"impression\"]\n",
        "\n",
        "    # Test label vector\n",
        "    test_sample_label = np.array(test_label_space.iloc[test_label_study_ids.index(test_study_id)][2:])\n",
        "\n",
        "    # Similar Search. Stage-1\n",
        "    # Return Top-<one_near_k_samples> samples' id based on label vector.  m is the size of our corpus. randomly sample from all training data.\n",
        "    near_study_ids = random_sample_selection_v2(test_sample_label, train_label_space, m=10000, k=one_near_k_samples)\n",
        "\n",
        "    # Get the similar report Findings and Impression based on 'near_study_ids'\n",
        "    near_samples = []\n",
        "    for index, train_row in train_data.iterrows():\n",
        "      if len(near_study_ids) == 0:\n",
        "        break\n",
        "      if int(train_row[\"study_id\"]) in near_study_ids:\n",
        "        near_samples.append({\"finding\": train_data.iloc[index][\"finding\"], \"impression\": train_data.iloc[index][\"impression\"]})\n",
        "        near_study_ids.remove(train_row[\"study_id\"])\n",
        "\n",
        "    # Similar Search. Stage-2\n",
        "    # Choose Top-<two_near_k_samples> samples from [near_samples] based on Rouge-1 score compared with test findings.\n",
        "    similar_samples = []\n",
        "    similar_samples_score = []\n",
        "    similar_score = []\n",
        "    for sample in near_samples:\n",
        "      s_scores = rouge.get_scores([test_findings], [sample[\"finding\"]])\n",
        "      similar_score.append(s_scores[0][rouge_type][\"f\"])\n",
        "\n",
        "    np_similar_score = np.array(similar_score)\n",
        "    similar_scores_index = np.argsort(-np_similar_score)\n",
        "    choose_index = similar_scores_index[:two_near_k_samples]\n",
        "\n",
        "    # Now we have some [similar_samples] filterd by two-stage Similar Search.\n",
        "    for c_index in choose_index:\n",
        "      similar_samples.append(near_samples[c_index])\n",
        "\n",
        "    good_reponse = []\n",
        "    bad_reponse = []\n",
        "    former_socre = 0\n",
        "    former_bad_socre = 0\n",
        "    all_response_score, all_response = [], []\n",
        "    try_count = 0\n",
        "    while True:\n",
        "      if len(good_reponse) == 0 and len(bad_reponse) == 0:\n",
        "        reponse = chestGPT_summary_near_sample_with_interactive_v6(test_findings, near_samples=similar_samples)\n",
        "      else:\n",
        "        reponse = chestGPT_summary_near_sample_with_interactive_v6(test_findings, near_samples=similar_samples, interactive=interactive, former_good_response=good_reponse, former_bad_response=bad_reponse)\n",
        "\n",
        "      if reponse == -2:\n",
        "        print(\"exceed length, pop 2 similar examples\")\n",
        "        # list_len = len(similar_samples)\n",
        "        for i in range(2):\n",
        "          similar_samples.pop()\n",
        "        continue\n",
        "\n",
        "      fotmatted_response = reponse.replace(\"IMPRESSION:\", \"\")\n",
        "      \n",
        "      compare_scores = []\n",
        "      for near_sa in similar_samples:\n",
        "        scores = rouge.get_scores([fotmatted_response], [near_sa[\"impression\"]])\n",
        "        single_score = scores[0][rouge_type][\"f\"]\n",
        "        compare_scores.append(single_score)\n",
        "\n",
        "      # print(compare_scores)\n",
        "      score = np.mean(np.array(compare_scores))\n",
        "      \n",
        "      all_response_score.append(score)\n",
        "      all_response.append(fotmatted_response)\n",
        "      try_count += 1\n",
        "      # if it is better than former\n",
        "      if score >= rouge_thre and score > former_socre:\n",
        "        former_socre = score\n",
        "        good_reponse.clear()  # Only keep one good response\n",
        "        good_reponse.append({\"summarize\": fotmatted_response, \"score\": score})\n",
        "      # if it is worse than former\n",
        "      if score < rouge_thre and score < former_socre:\n",
        "        former_bad_socre = score\n",
        "        # Keep more than one bad response. Length limit is 8 here.\n",
        "        if len(bad_reponse) > 8: \n",
        "          bad_reponse = bad_reponse[-8:]    \n",
        "        # bad_reponse.clear()  \n",
        "        bad_reponse.append({\"summarize\": fotmatted_response, \"score\": score})\n",
        "      if try_count > interactive_times:\n",
        "        break\n",
        "\n",
        "    max_score_index = all_response_score.index(max(all_response_score))\n",
        "    max_score = all_response_score[max_score_index]\n",
        "    final_best_response = all_response[max_score_index]\n",
        "    # print(\"max score=\", max_score, \"\\n\", final_best_response)\n",
        "    # print(\"max score=\", max_score, \"\\n\")\n",
        "    return row_index, final_best_response\n",
        "\n",
        "\n",
        "# 'mid_path' is the experiment name. InteractV6: Prompt version. 2Snear: Two-stage similar selection.  \n",
        "mid_path = \"InteractV6_2Snear{}-{}_1Gd8Bds_i{}r{}_{}\".format(one_near_k_samples, two_near_k_samples, interactive_times, rouge_thre, rouge_type)\n",
        "save_root = osp.join(root, mid_path)\n",
        "if not osp.exists(save_root):\n",
        "  os.makedirs(save_root) \n",
        "print(save_root)\n",
        "\n",
        "# To avoid accidental interruption of the program and loss of results, I split it into multiple patches.\n",
        "for loop_index in range(7):\n",
        "  print(time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "  Test_start = loop_index * 250\n",
        "  Test_end = (loop_index + 1) * 250\n",
        "\n",
        "  if Test_end > len(test_label_space):\n",
        "    Test_end = len(test_label_space)\n",
        "  print(Test_start, Test_end)\n",
        "\n",
        "  # load test data\n",
        "  df_test_csv = pd.read_csv('baseline_sections_test_V2_Clean.csv')\n",
        "  df_test_csv = df_test_csv[Test_start:Test_end]\n",
        "\n",
        "  # parallel\n",
        "  with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    # time.sleep(20)\n",
        "    results = dict(tqdm(executor.map(process_row_with_random_choice_samples, df_test_csv.iterrows()), total=len(df_test_csv)))\n",
        "\n",
        "  # same order as the input\n",
        "  df_test_csv['summary'] = df_test_csv.index.map(results.get)\n",
        "\n",
        "  # out_file_name is name of split csv\n",
        "  out_file_name = \"test{}_{}_IntactV6_2Snear{}-{}_1Gd8Bds_i{}r{}_{}.csv\".format(Test_start, Test_end, one_near_k_samples, two_near_k_samples, interactive_times, rouge_thre, rouge_type)\n",
        "  output_file = osp.join(save_root, \"{}\".format(out_file_name))\n",
        "  df_test_csv.to_csv(output_file, index=False)\n",
        "\n",
        "  rouge_e = evaluate.load('rouge')\n",
        "\n",
        "  rouge_scores = []\n",
        "  for index, row in df_test_csv.iterrows():\n",
        "      label = row['impression']\n",
        "      prediction = row['summary']\n",
        "      prediction = prediction.replace(\"IMPRESSION:\", \"\")\n",
        "      score = rouge_e.compute(predictions=[prediction], references=[label], rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
        "      rouge_scores.append(score)\n",
        "\n",
        "  # mean\n",
        "  mean_rouge1 = sum([score['rouge1'] for score in rouge_scores]) / len(rouge_scores)\n",
        "  mean_rouge2 = sum([score['rouge2'] for score in rouge_scores]) / len(rouge_scores)\n",
        "  mean_rougeL = sum([score['rougeL'] for score in rouge_scores]) / len(rouge_scores)\n",
        "\n",
        "  print(f'R-1: {mean_rouge1:.4f}', f',R-2: {mean_rouge2:.4f}', f',R-L: {mean_rougeL:.4f}')\n",
        "\n",
        "\n",
        "# Final test for all csv data\n",
        "\n",
        "print(\"### Final Test for All CSVs ###\")\n",
        "\n",
        "rouge_e = evaluate.load('rouge')\n",
        "\n",
        "contact_list = []\n",
        "range_list = [0, 250, 500, 750, 1000, 1250, 1500, 1603]\n",
        "for loop_index in range(len(range_list)-1):\n",
        "  Test_start = range_list[loop_index]\n",
        "  Test_end = range_list[loop_index + 1]\n",
        "  out_file_name = \"test{}_{}_IntactV6_2Snear{}-{}_1Gd8Bds_i{}r{}_{}.csv\".format(Test_start, Test_end, one_near_k_samples, two_near_k_samples, interactive_times, rouge_thre, rouge_type)\n",
        "  test_pd = pd.read_csv(osp.join(save_root, out_file_name))\n",
        "  contact_list.append(test_pd)\n",
        "\n",
        "df_save_csv = pd.concat(contact_list)\n",
        "df_save_csv.to_csv(osp.join(save_root, \"testAll_IntactV6_2Snear{}-{}_1Gd8Bds_i{}r{}_{}.csv\".format(one_near_k_samples, two_near_k_samples, interactive_times, rouge_thre, rouge_type)), index=False)\n",
        "\n",
        "\n",
        "rouge_scores = []\n",
        "for index, row in df_save_csv.iterrows():\n",
        "    label = row['impression']\n",
        "    prediction = row['summary']\n",
        "    prediction = prediction.replace(\"IMPRESSION:\", \"\")\n",
        "    score = rouge_e.compute(predictions=[prediction], references=[label], rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
        "    rouge_scores.append(score)\n",
        "\n",
        "\n",
        "# mean\n",
        "mean_rouge1 = sum([score['rouge1'] for score in rouge_scores]) / len(rouge_scores)\n",
        "mean_rouge2 = sum([score['rouge2'] for score in rouge_scores]) / len(rouge_scores)\n",
        "mean_rougeL = sum([score['rougeL'] for score in rouge_scores]) / len(rouge_scores)\n",
        "\n",
        "print(f'R-1: {mean_rouge1:.4f}', f'R-2: {mean_rouge2:.4f}', f'R-L: {mean_rougeL:.4f}')\n",
        "\n"
      ],
      "metadata": {
        "id": "K_F79UJY1T_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **One Good And N Bad**\n",
        "\n",
        "# OpenI data \n"
      ],
      "metadata": {
        "id": "OoZ0eJzH5UZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge import Rouge\n",
        "\n",
        "rouge = Rouge()\n",
        "\n",
        "root = \"\"\n",
        "\n",
        "train_data = pd.read_csv(\"train576_data.csv\")\n",
        "test_label_space = pd.read_csv(\"test576_label.csv\")\n",
        "train_label_space = pd.read_csv(\"train576_label.csv\")\n",
        "\n",
        "train_len = len(train_label_space)\n",
        "test_label_study_ids = []\n",
        "for index, row in test_label_space.iterrows():\n",
        "  test_label_study_ids.append(row[\"ids\"])\n",
        "\n",
        "# use_near_sample = False\n",
        "# use_near_sample = True\n",
        "interactive = True\n",
        "interactive_times = 20\n",
        "rouge_thre = 0.85\n",
        "one_near_k_samples = 200\n",
        "two_near_k_samples = 16\n",
        "rouge_type = \"rouge-1\"\n",
        "\n",
        "\n",
        "#chestGPT_summary_near_sample_with_interactive(text, near_samples=None, interactive=False, former_good_response=None, former_bad_response=None):\n",
        "print(\"Start at\", time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "def process_row_with_random_choice_samples(args):\n",
        "    row_index, row = args\n",
        "\n",
        "    test_id = row['ids']\n",
        "    test_findings, test_impression = row[\"finding\"], row[\"impression\\n\"]\n",
        "\n",
        "    test_sample_label = np.array(test_label_space.iloc[test_label_study_ids.index(test_id)][1:])\n",
        "\n",
        "    near_study_ids = random_sample_selection_v2_for_openI(test_sample_label, train_label_space, m=train_len, k=one_near_k_samples)\n",
        "\n",
        "    near_samples = []\n",
        "    for index, train_row in train_data.iterrows():\n",
        "      if len(near_study_ids) == 0:\n",
        "        break\n",
        "      if int(train_row[\"ids\"]) in near_study_ids:\n",
        "        near_samples.append({\"finding\": train_data.iloc[index][\"finding\"], \"impression\": train_data.iloc[index][\"impression\\n\"]})\n",
        "        near_study_ids.remove(train_row[\"ids\"])\n",
        "\n",
        "\n",
        "    similar_samples = []\n",
        "    similar_samples_score = []\n",
        "    similar_score = []\n",
        "    for sample in near_samples:\n",
        "      # s_scores = rouge_e.compute(predictions=[test_findings], references=[sample[\"finding\"]], rouge_types=[rouge_type], use_stemmer=True)\n",
        "      # similar_score.append(s_scores[rouge_type])\n",
        "      \n",
        "      s_scores = rouge.get_scores([test_findings], [sample[\"finding\"]])\n",
        "      similar_score.append(s_scores[0][rouge_type][\"f\"])\n",
        "\n",
        "    np_similar_score = np.array(similar_score)\n",
        "    similar_scores_index = np.argsort(-np_similar_score)\n",
        "    choose_index = similar_scores_index[:two_near_k_samples]\n",
        "\n",
        "    for c_index in choose_index:\n",
        "      similar_samples.append(near_samples[c_index])\n",
        "\n",
        "\n",
        "    good_reponse = []\n",
        "    bad_reponse = []\n",
        "    former_socre = 0\n",
        "    former_bad_socre = Math.inf\n",
        "    all_response_score, all_response = [], []\n",
        "    try_count = 0\n",
        "    while True:\n",
        "      if len(good_reponse) == 0 and len(bad_reponse) == 0:\n",
        "        reponse = chestGPT_summary_near_sample_with_interactive_v6(test_findings, near_samples=similar_samples)\n",
        "      else:\n",
        "        reponse = chestGPT_summary_near_sample_with_interactive_v6(test_findings, near_samples=similar_samples, interactive=interactive, former_good_response=good_reponse, former_bad_response=bad_reponse)\n",
        "\n",
        "      if reponse == -2:\n",
        "        print(\"exceed length, pop the similar examples\")\n",
        "        # list_len = len(similar_samples)\n",
        "        for i in range(2):\n",
        "          similar_samples.pop()\n",
        "        continue\n",
        "      fotmatted_response = reponse.replace(\"IMPRESSION:\", \"\")\n",
        "      \n",
        "      compare_scores = []\n",
        "      for near_sa in similar_samples:\n",
        "        scores = rouge.get_scores([fotmatted_response], [near_sa[\"impression\"]])\n",
        "        single_score = scores[0][rouge_type][\"f\"]\n",
        "        compare_scores.append(single_score)\n",
        "\n",
        "      # print(compare_scores)\n",
        "      score = np.mean(np.array(compare_scores))\n",
        "      \n",
        "      all_response_score.append(score)\n",
        "      all_response.append(fotmatted_response)\n",
        "      try_count += 1\n",
        "      if score >= rouge_thre and score > former_socre:\n",
        "        former_socre = score\n",
        "        good_reponse.clear()\n",
        "        good_reponse.append({\"summarize\": fotmatted_response, \"score\": score})\n",
        "      # if score < rouge_thre - 0.1 and score < former_socre:\n",
        "      if score < rouge_thre and score < former_bad_socre:\n",
        "        former_bad_socre = score\n",
        "        if len(bad_reponse) > 8:\n",
        "          bad_reponse = bad_reponse[-8:]    \n",
        "        # bad_reponse.clear()\n",
        "        bad_reponse.append({\"summarize\": fotmatted_response, \"score\": score})\n",
        "      if try_count > interactive_times:\n",
        "        break\n",
        "\n",
        "    max_score_index = all_response_score.index(max(all_response_score))\n",
        "    max_score = all_response_score[max_score_index]\n",
        "    final_best_response = all_response[max_score_index]\n",
        "    # print(\"max score=\", max_score, \"\\n\", final_best_response)\n",
        "    # print(\"max score=\", max_score, \"\\n\")\n",
        "    return row_index, final_best_response\n",
        "\n",
        "\n",
        "mid_path = \"InteractV6_2Snear{}-{}_1Gd8Bds_i{}r{}_{}\".format(one_near_k_samples, two_near_k_samples, interactive_times, rouge_thre, rouge_type)\n",
        "save_root = osp.join(root, mid_path)\n",
        "if not osp.exists(save_root):\n",
        "  os.makedirs(save_root) \n",
        "print(save_root)\n",
        "\n",
        "\n",
        "for loop_index in range(3):\n",
        "  print(time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
        "  Test_start = loop_index * 200\n",
        "  Test_end = (loop_index + 1) * 200\n",
        "\n",
        "  df_test_csv = pd.read_csv(\"test576_data.csv\")\n",
        "  if Test_end > len(df_test_csv):\n",
        "    Test_end = len(df_test_csv)\n",
        "\n",
        "  print(Test_start, Test_end)\n",
        "  df_test_csv = df_test_csv[Test_start:Test_end]\n",
        "\n",
        "  # parallel\n",
        "  with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "    # time.sleep(20)\n",
        "    results = dict(tqdm(executor.map(process_row_with_random_choice_samples, df_test_csv.iterrows()), total=len(df_test_csv)))\n",
        "\n",
        "  # same order as the input\n",
        "  df_test_csv['summary'] = df_test_csv.index.map(results.get)\n",
        "\n",
        "  out_file_name = \"test{}_{}_IntactV6_2Snear{}-{}_1Gd8Bds_i{}r{}_{}.csv\".format(Test_start, Test_end, one_near_k_samples, two_near_k_samples, interactive_times, rouge_thre, rouge_type)\n",
        "  output_file = osp.join(save_root, \"{}\".format(out_file_name))\n",
        "  df_test_csv.to_csv(output_file, index=False)\n",
        "\n",
        "  rouge_e = evaluate.load('rouge')\n",
        "\n",
        "  rouge_scores = []\n",
        "  for index, row in df_test_csv.iterrows():\n",
        "      label = row['impression\\n']\n",
        "      prediction = row['summary']\n",
        "      prediction = prediction.replace(\"IMPRESSION:\", \"\")\n",
        "      score = rouge_e.compute(predictions=[prediction], references=[label], rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
        "      rouge_scores.append(score)\n",
        "\n",
        "  # mean\n",
        "  mean_rouge1 = sum([score['rouge1'] for score in rouge_scores]) / len(rouge_scores)\n",
        "  mean_rouge2 = sum([score['rouge2'] for score in rouge_scores]) / len(rouge_scores)\n",
        "  mean_rougeL = sum([score['rougeL'] for score in rouge_scores]) / len(rouge_scores)\n",
        "\n",
        "  print(f'R-1: {mean_rouge1:.4f}', f',R-2: {mean_rouge2:.4f}', f',R-L: {mean_rougeL:.4f}')\n",
        "\n",
        "\n",
        "\n",
        "# Final test for all csv data\n",
        "\n",
        "print(\"### Final Test for All CSVs ###\")\n",
        "\n",
        "rouge_e = evaluate.load('rouge')\n",
        "\n",
        "contact_list = []\n",
        "range_list = [0, 200, 400, 576]\n",
        "for loop_index in range(len(range_list)-1):\n",
        "  Test_start = range_list[loop_index]\n",
        "  Test_end = range_list[loop_index + 1]\n",
        "  out_file_name = \"test{}_{}_IntactV6_2Snear{}-{}_1Gd8Bds_i{}r{}_{}.csv\".format(Test_start, Test_end, one_near_k_samples, two_near_k_samples, interactive_times, rouge_thre, rouge_type)\n",
        "  test_pd = pd.read_csv(osp.join(save_root, out_file_name))\n",
        "  contact_list.append(test_pd)\n",
        "\n",
        "df_save_csv = pd.concat(contact_list)\n",
        "df_save_csv.to_csv(osp.join(save_root, \"testAll_IntactV6_2Snear{}-{}_1Gd8Bds_i{}r{}_{}.csv\".format(one_near_k_samples, two_near_k_samples, interactive_times, rouge_thre, rouge_type)), index=False)\n",
        "\n",
        "\n",
        "rouge_scores = []\n",
        "for index, row in df_save_csv.iterrows():\n",
        "    label = row['impression']\n",
        "    prediction = row['summary']\n",
        "    prediction = prediction.replace(\"IMPRESSION:\", \"\")\n",
        "    score = rouge_e.compute(predictions=[prediction], references=[label], rouge_types=[\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
        "    rouge_scores.append(score)\n",
        "\n",
        "\n",
        "# mean\n",
        "mean_rouge1 = sum([score['rouge1'] for score in rouge_scores]) / len(rouge_scores)\n",
        "mean_rouge2 = sum([score['rouge2'] for score in rouge_scores]) / len(rouge_scores)\n",
        "mean_rougeL = sum([score['rougeL'] for score in rouge_scores]) / len(rouge_scores)\n",
        "\n",
        "print(f'R-1: {mean_rouge1:.4f}', f'R-2: {mean_rouge2:.4f}', f'R-L: {mean_rougeL:.4f}')\n",
        "\n"
      ],
      "metadata": {
        "id": "mvtoLqxj5XXw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "JGK3_tZjrecY",
        "Nbf9THTwG8qH",
        "5EqoyNwWpGd0",
        "qQ4uD6kZsAci",
        "IuHRE0ir1dNw",
        "oAE2l9sK1MuS",
        "OoZ0eJzH5UZM"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
